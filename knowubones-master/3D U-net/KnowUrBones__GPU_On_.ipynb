{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add abstract + Introduction about the project\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part pre-processing the data  (Very painful part..)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 1.**Generate TFRecords files**\n",
    "\n",
    "we are using TFRecords to define our dataset because of two key elements:\n",
    "-First this is a tensorflow format so more efficient with tf, easier to work direclty with tf\n",
    "-Since we work we large dataset TFRecords allows us to dereclty stream the data over the network, do not need to load it into the code.\n",
    "\n",
    "\n",
    "## 2.**Processing the data**\n",
    "\n",
    "All transformations that your dataset required\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basics functions \n",
    "those functions converting our data to feature for TFRecords files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def _bytes_features(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_edge(data):\n",
    "    \n",
    "    \n",
    "    D,H,W,_ = data.shape\n",
    "    D_s, D_e = 0, D-1\n",
    "    H_s, H_e = 0, H-1\n",
    "    W_s, W_e = 0, W-1\n",
    "    \n",
    "    \n",
    "    while D_s < D:\n",
    "        if data[D_s].sum() != 0:\n",
    "            break\n",
    "        D_s += 1\n",
    "    \n",
    "    while D_e > D_s:\n",
    "        if data[D_e].sum() !=0:\n",
    "            break\n",
    "        D_e -=1\n",
    "        \n",
    "    while H_s < H:\n",
    "        if data[:,H_s].sum() != 0:\n",
    "            break\n",
    "        H_s += 1\n",
    "    \n",
    "    while H_e > H_s:\n",
    "        if data[:,H_e].sum() !=0:\n",
    "            break\n",
    "        H_e -=1\n",
    "        \n",
    "    while W_s < W:\n",
    "        if data[:,:,W_s].sum() != 0:\n",
    "            break\n",
    "        W_s += 1\n",
    "    \n",
    "    while W_e > W_s:\n",
    "        if data[:,:,W_e].sum() !=0:\n",
    "            break\n",
    "        W_e -=1\n",
    "    \n",
    "    original_shape=[D,H,W]\n",
    "    cut_size = [int(D_s),int(D_e+1),int(H_s),int(H_e+1),int(W_s),int(W_e+1)]\n",
    "    return(original_shape,cut_size)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_subject(raw_data_dir,subject_id):\n",
    "    \n",
    "    \n",
    "    subject_name = 'HeadAndNeck_anonymous-%d' %subject_id\n",
    "    path_ct=os.path.join(raw_data_dir, subject_name+'.mha')\n",
    "    \n",
    "    img_ct = sitk.ReadImage(path_ct)\n",
    "    img_ct_array = sitk.GetArrayFromImage(img_ct)\n",
    "    D,H,W=img_ct_array.shape\n",
    "    img_ct_array=np.resize(img_ct_array,[D-1,H,W])\n",
    "    \n",
    "    if subject_id < 17:\n",
    "        path_ct_lbl=os.path.join(raw_data_dir, subject_name+'-label.mha')\n",
    "        lbl_ct = sitk.ReadImage(path_ct_lbl)\n",
    "        label_ct_array = sitk.GetArrayFromImage(lbl_ct)\n",
    "        D,H,W=label_ct_array.shape\n",
    "        label_ct_array=np.resize(label_ct_array,[D-1,H,W])\n",
    "    else:\n",
    "        label_ct_array=None\n",
    "    \n",
    "    return[img_ct_array[...,np.newaxis],label_ct_array[...,np.newaxis]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation(cutted_image, patch_size, overlap_stepsize):\n",
    "    \n",
    "    patch_ids = []\n",
    "    \n",
    "    D,H,W,_ = cutted_image.shape\n",
    "    \n",
    "    drange = list(range(0,D-patch_size+1, overlap_stepsize))\n",
    "    hrange = list(range(0,H-patch_size+1, overlap_stepsize))\n",
    "    wrange = list(range(0,W-patch_size+1, overlap_stepsize))\n",
    "    \n",
    "    \n",
    "    if (D-patch_size) % overlap_stepsize != 0:\n",
    "        drange.append(D-patch_size)\n",
    "\n",
    "    if (H-patch_size) % overlap_stepsize != 0:\n",
    "        hrange.append(H-patch_size)\n",
    "\n",
    "    if (W-patch_size) % overlap_stepsize != 0:\n",
    "        wrange.append(W-patch_size)\n",
    "        \n",
    "    \n",
    "    for d in drange:\n",
    "        for h in hrange:\n",
    "            for w in wrange:\n",
    "                patch_ids.append((d,h,w))\n",
    "                \n",
    "                \n",
    "    return(patch_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_training_examples(ct_img,ct_lbl, original_shape, cut_size, output_file):\n",
    "    \n",
    "    \n",
    "    writer = tf.python_io.TFRecordWriter(output_file)\n",
    "    example= tf.train.Example(features=tf.train.Features(\n",
    "    feature={'ct':_bytes_features([ct_img[:,:,:,0].tostring()]),\n",
    "             'label':_bytes_features([ct_lbl[:,:,:,0].tostring()]),\n",
    "             'cut_size':_int64_feature(cut_size),\n",
    "             'original_shape':_int64_feature(original_shape),\n",
    "                \n",
    "    }\n",
    "    \n",
    "    ))\n",
    "    \n",
    "    \n",
    "    writer.write(example.SerializeToString())\n",
    "    writer.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_validation_examples(ct_img, lbl_img, patch_size , cut_size, overlap_stepsize, output_file):\n",
    "    \n",
    "    \n",
    "    ct_img=ct_img[cut_size[0]:cut_size[1],cut_size[2]:cut_size[3],cut_size[4]:cut_size[5],:]\n",
    "    lbl_img=lbl_img[cut_size[0]:cut_size[1],cut_size[2]:cut_size[3],cut_size[4]:cut_size[5],:]\n",
    "    \n",
    "    patch_ids = prepare_validation(ct_img,patch_size,overlap_stepsize)\n",
    "    print('Number of patches:', len(patch_ids))\n",
    "    \n",
    "    \n",
    "    writer = tf.python_io.TFRecordWriter(output_file)\n",
    "    \n",
    "    for i in range(len(patch_ids)):\n",
    "        (d,h,w) = patch_ids[i]\n",
    "        \n",
    "        _ct_img = ct_img[d:d+patch_size,h:h+patch_size,w:w+patch_size,:]\n",
    "        _lbl_img = lbl_img[d:d+patch_size,h:h+patch_size,w:w+patch_size,:]\n",
    "        \n",
    "        \n",
    "        example = tf.train.Example(features=tf.train.Features(\n",
    "        feature={'ct':_bytes_features([_ct_img[:,:,:,0].tostring()]),\n",
    "                 'label':_bytes_features([_lbl_img[:,:,:,0].tostring()])\n",
    "                 \n",
    "                 \n",
    "            \n",
    "        }\n",
    "        ))\n",
    "        \n",
    "        writer.write(example.SerializeToString())\n",
    "    \n",
    "    writer.close()\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_prediction_examples(ct_img,patch_size, cut_size, overlap_stepsize, output_file):\n",
    "    \n",
    "    ct_img=ct_img[cut_size[0]:cut_size[1],cut_size[2]:cut_size[3],cut_size[4]:cut_size[5],:]\n",
    "    \n",
    "    patch_ids = prepare_validation(ct_img, patch_size, overlap_stepsize)\n",
    "    print('Number of patches',len(patch_ids))\n",
    "    \n",
    "    writer = tf.python_io.TFRecordWriter(output_file)\n",
    "    \n",
    "    for i in range(len(patch_ids)):\n",
    "        (d,h,w) = patch_ids[i]\n",
    "        \n",
    "        _ct_img = ct_img[d:d+patch_size,h:h+patch_size,w:w+patch_size,:]\n",
    "        \n",
    "        \n",
    "        \n",
    "        example = tf.train.Example(features=tf.train.Features(\n",
    "        feature={'ct':_bytes_features([_ct_img[:,:,:,0].tostring()]) \n",
    "            \n",
    "        }\n",
    "        ))\n",
    "        \n",
    "        writer.write(example.SerializeToString())\n",
    "    \n",
    "    writer.close()\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_files(raw_data_dir, output_path,valid_id, pred_id, patch_size,overlap_stepsize):\n",
    "    number_of_patients=8\n",
    "    \n",
    "    if valid_id not in range(1,number_of_patients+1) and valid_id !=-1:\n",
    "        print('the valid_id should be between [1, %d]' %number_of_patients)\n",
    "        sys.exit(-1)\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "        \n",
    "    for i in range (1,number_of_patients+1):\n",
    "        print('----Process subject %d:---' % i)\n",
    "        \n",
    "        subject_name = 'subject-%d' %i\n",
    "        train_filename = os.path.join(output_path, subject_name+'.tfrecords')\n",
    "        \n",
    "        pred_subject_name = 'subject-%d-pred-%d-patch-%d' % (pred_id, overlap_stepsize, patch_size)\n",
    "        pred_filename = os.path.join(output_path, pred_subject_name+'.tfrecords')\n",
    "        \n",
    "        valid_subject_name = 'subject-%d-valid-%d-patch-%d' %  (valid_id, overlap_stepsize, patch_size)\n",
    "        valid_filename = os.path.join(output_path, valid_subject_name+'.tfrecords')\n",
    "        \n",
    "        converted_label_filename = 'subject-%d-label.npy' % valid_id\n",
    "        converted_label_filename = os.path.join(output_path, converted_label_filename)\n",
    "        \n",
    "        if (i < number_of_patients+1  and not os.path.isfile(train_filename)) or \\\n",
    "            (i == pred_id and not os.path.isfile(pred_filename)) or \\\n",
    "            (i == valid_id and (not os.path.isfile (valid_filename) or \\\n",
    "                not os.path.isfile(converted_label_filename))):\n",
    "                print('Loading data...')\n",
    "                \n",
    "                [ct,lbl] = load_subject(raw_data_dir, i)\n",
    "                \n",
    "                if lbl is not None:\n",
    "                    print('Check label', np.unique(lbl),'Total number of labels',len(np.unique(lbl)))\n",
    "                    \n",
    "                    \n",
    "                [original_shape, cut_size] = cut_edge(ct)\n",
    "                print('check original size', original_shape)\n",
    "                print('check cut size', cut_size)\n",
    "                \n",
    "        \n",
    "        if not os.path.isfile(train_filename) and i < number_of_patients+1:\n",
    "            print('create the training file')\n",
    "            write_training_examples(ct,lbl,original_shape,cut_size, train_filename)\n",
    "            \n",
    "        if i == valid_id:\n",
    "            if not os.path.isfile(valid_filename):\n",
    "                print('Create the validation file:')\n",
    "                write_validation_examples(ct,lbl, patch_size, cut_size, overlap_stepsize ,valid_filename)\n",
    "                \n",
    "            if not os.path.isfile(pred_filename):\n",
    "                print('Create the converted label file:')\n",
    "                np.save(converted_label_filename, lbl[:,:,:,0])\n",
    "                \n",
    "        if i == pred_id:\n",
    "            if not os.path.isfile(pred_filename):\n",
    "                print('Create the prediction file')\n",
    "                write_prediction_examples(ct,patch_size,cut_size,overlap_stepsize, pred_filename)\n",
    "                \n",
    "        print('---Done---')\n",
    "                \n",
    "        \n",
    "                \n",
    "        \n",
    "\n",
    "                \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir=r\"C:\\Users\\r778e\\Desktop\\Data_to_aug\\data_set_no_augmentation\"\n",
    "output_path=r\"C:\\Users\\r778e\\Desktop\\tfrecord\"\n",
    "valid_id=3\n",
    "pred_id=5\n",
    "patch_size=32\n",
    "overlap_stepsize=20\n",
    "generate_files(raw_data_dir, output_path,valid_id, pred_id, patch_size,overlap_stepsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames(data_dir,mode,valid_id, pred_id, overlap_step, patch_size):\n",
    "    number_of_patients=8\n",
    "    if mode == 'train':\n",
    "        train_files = [os.path.join(data_dir, 'subject-%d.tfrecords' % i) for i in range(1, number_of_patients+1) if i != valid_id]\n",
    "        \n",
    "        \n",
    "        for f in train_files:\n",
    "            assert os.path.isfile(f), \\\n",
    "            ('Run generate_tfrecord.py to generate training files.')\n",
    "        return train_files\n",
    "    elif mode == 'valid':\n",
    "        valid_file = os.path.join(data_dir, 'subject-%d-valid-%d-patch-%d.tfrecords' % (valid_id, overlap_step, patch_size ))\n",
    "        assert os.path.isfile(valid_file), \\\n",
    "            ('Check generate_tfrecords function to generate the validation file')\n",
    "        return [valid_file]\n",
    "    elif mode == 'pred':\n",
    "        pred_file = os.path.join(data_dir, 'subject-%d-pred-%d-patch-%d.tfrecords' %(pred_id, overlap_step, patch_size))\n",
    "        assert os.path.isfile(pred_file), \\\n",
    "           ('Check generate_tfrecords function to generate the prediction file')\n",
    "        return [pred_file]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_train(serialized_example):\n",
    "    \n",
    "    features = tf.parse_single_example(serialized_example,\n",
    "                                      features={\n",
    "                                          'ct':tf.FixedLenFeature([],tf.string),\n",
    "                                       'label':tf.FixedLenFeature([],tf.string),\n",
    "                                          'cut_size':tf.FixedLenFeature(6,tf.int64),\n",
    "                                          'original_shape':tf.FixedLenFeature(3,tf.int64)\n",
    "     \n",
    "                                      \n",
    "                                      })\n",
    "    \n",
    "    img_shape = features['original_shape']\n",
    "    cut_size = features['cut_size']\n",
    "    \n",
    "    image_ct=tf.decode_raw(features['ct'],tf.int16)\n",
    "    image_ct = tf.reshape(image_ct, img_shape)\n",
    "    \n",
    "    \n",
    "    label_ct=tf.decode_raw(features['label'],tf.uint16)\n",
    "    label_ct = tf.reshape(label_ct, img_shape)\n",
    "\n",
    "    \n",
    "    \n",
    "    image_ct= tf.cast(image_ct, tf.float32)\n",
    "    label_ct= tf.cast(label_ct, tf.float32)\n",
    "\n",
    "    \n",
    "    return image_ct, label_ct,cut_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_valid(serialized_example):\n",
    "    patch_size=32\n",
    "    features = tf.parse_single_example(serialized_example,\n",
    "                                      features={\n",
    "                                          'ct':tf.FixedLenFeature([],tf.string),\n",
    "                                       'label':tf.FixedLenFeature([],tf.string),\n",
    "                                     \n",
    "            \n",
    "                                      })\n",
    "    \n",
    "    patch_shape=[patch_size]*3\n",
    "    image_ct=tf.decode_raw(features['ct'],tf.int16)\n",
    "    image_ct = tf.reshape(image_ct, patch_shape)\n",
    "  \n",
    "    label_ct=tf.decode_raw(features['label'],tf.uint16)\n",
    "    label_ct = tf.reshape(label_ct, patch_shape)\n",
    "\n",
    "    \n",
    "    \n",
    "    image_ct= tf.cast(image_ct, tf.float32)\n",
    "    label_ct= tf.cast(label_ct, tf.float32)\n",
    "\n",
    "    \n",
    "    return image_ct, label_ct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_pred(serialized_example):\n",
    "    patch_size=32\n",
    "    features = tf.parse_single_example(serialized_example,\n",
    "                                      features={\n",
    "                                          'ct':tf.FixedLenFeature([],tf.string),\n",
    "\n",
    "        \n",
    "                                      \n",
    "                                    })\n",
    "    \n",
    "    \n",
    "    patch_shape=[patch_size]*3\n",
    "    \n",
    "    image_ct=tf.decode_raw(features['ct'],tf.int16)\n",
    "    image_ct = tf.reshape(image_ct, patch_shape)\n",
    "    \n",
    "\n",
    "    \n",
    "    image_ct= tf.cast(image_ct, tf.float32)\n",
    "    label_ct = tf.zeros(image_ct.shape)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return image_ct, label_ct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(image_ct,label_ct,cut_size):\n",
    "    \n",
    "    patch_size=32\n",
    "    data= tf.stack([image_ct,label_ct],axis=-1)\n",
    "    \n",
    "    \n",
    "    image = tf.random_crop(data[cut_size[0]:cut_size[1], cut_size[2]:cut_size[3], cut_size[4]:cut_size[5],:],[patch_size,patch_size,patch_size,2])\n",
    "\n",
    "    [image_ct, lbl_image] = tf.unstack(image,2,axis=-1)\n",
    "    \n",
    "    return image_ct, lbl_image\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image(image_ct,label_ct):\n",
    "    \n",
    "    image_ct = tf.image.per_image_standardization(image_ct)\n",
    "  \n",
    "    \n",
    "    feature = tf.cast(image_ct, tf.float32)\n",
    "    label = tf.cast(label_ct, tf.int32)\n",
    "    \n",
    "    return feature,label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_function(data_dir, mode,valid_id, pred_id ,overlap_step , patch_size, batch_size, buffer_size ,num_epochs=1, num_parallel_calls=1):\n",
    "    \n",
    "    \n",
    "    \n",
    "    with tf.name_scope('input'):\n",
    "        \n",
    "        filenames = get_filenames(data_dir, mode, valid_id, pred_id, overlap_step, patch_size)\n",
    "        dataset = tf.data.TFRecordDataset(filenames)\n",
    "        \n",
    "        \n",
    "        dataset = dataset.prefetch(buffer_size=batch_size)\n",
    "        \n",
    "        \n",
    "        if mode == 'train':\n",
    "            \n",
    "            dataset = dataset.shuffle(buffer_size=buffer_size)\n",
    "            \n",
    "            \n",
    "            \n",
    "        dataset = dataset.repeat(num_epochs)\n",
    "        \n",
    "        if mode == 'train':\n",
    "            dataset = dataset.map(decode_train, num_parallel_calls=num_parallel_calls)\n",
    "            dataset = dataset.map(crop_image, num_parallel_calls=num_parallel_calls)\n",
    "            \n",
    "        elif mode == 'valid':\n",
    "            dataset = dataset.map(decode_valid, num_parallel_calls=num_parallel_calls)\n",
    "        elif mode == 'pred':\n",
    "            dataset = dataset.map(decode_pred , num_parallel_calls=num_parallel_calls)\n",
    "        \n",
    "        \n",
    "        dataset = dataset.map(normalize_image, num_parallel_calls=num_parallel_calls)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        \n",
    "        \n",
    "        dataset.prefetch(1)\n",
    "        \n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        feature, label = iterator.get_next()\n",
    "        \n",
    "        return tf.expand_dims(feature,-1), tf.cast(label, tf.int32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of operations that will be use to built the differents blocks\n",
    "\n",
    "### All operations defined are axecuted on 5D tensor  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "################################################################################\n",
    "\n",
    "# Basic operations building the network\n",
    "\n",
    "################################################################################\n",
    "\n",
    "\n",
    "\n",
    "def Pool3d(inputs, kernel_size, strides):\n",
    "\n",
    "\t\"\"\"Performs 3D max pooling.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\treturn tf.layers.max_pooling3d(\n",
    "\n",
    "\t\t\tinputs=inputs,\n",
    "\n",
    "\t\t\tpool_size=kernel_size,\n",
    "\n",
    "\t\t\tstrides=strides,\n",
    "\n",
    "\t\t\tpadding='same')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Deconv3D(inputs, filters, kernel_size, strides, use_bias=False):\n",
    "\n",
    "\t\"\"\"Performs 3D deconvolution without bias and activation function.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\treturn tf.layers.conv3d_transpose(\n",
    "\n",
    "\t\t\tinputs=inputs,\n",
    "\n",
    "\t\t\tfilters=filters,\n",
    "\n",
    "\t\t\tkernel_size=kernel_size,\n",
    "\n",
    "\t\t\tstrides=strides,\n",
    "\n",
    "\t\t\tpadding='same',\n",
    "\n",
    "\t\t\tuse_bias=use_bias,\n",
    "\n",
    "\t\t\tkernel_initializer=tf.truncated_normal_initializer())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Conv3D(inputs, filters, kernel_size, strides, use_bias=False):\n",
    "\n",
    "\t\"\"\"Performs 3D convolution without bias and activation function.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\treturn tf.layers.conv3d(\n",
    "\n",
    "\t\t\tinputs=inputs,\n",
    "\n",
    "\t\t\tfilters=filters,\n",
    "\n",
    "\t\t\tkernel_size=kernel_size,\n",
    "\n",
    "\t\t\tstrides=strides,\n",
    "\n",
    "\t\t\tpadding='same',\n",
    "\n",
    "\t\t\tuse_bias=use_bias,\n",
    "\n",
    "\t\t\tkernel_initializer=tf.truncated_normal_initializer())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Dilated_Conv3D(inputs, filters, kernel_size, dilation_rate, use_bias=False):\n",
    "\n",
    "\t\"\"\"Performs 3D dilated convolution without bias and activation function.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\treturn tf.layers.conv3d(\n",
    "\n",
    "\t\t\tinputs=inputs,\n",
    "\n",
    "\t\t\tfilters=filters,\n",
    "\n",
    "\t\t\tkernel_size=kernel_size,\n",
    "\n",
    "\t\t\tstrides=1,\n",
    "\n",
    "\t\t\tdilation_rate=dilation_rate,\n",
    "\n",
    "\t\t\tpadding='same',\n",
    "\n",
    "\t\t\tuse_bias=use_bias,\n",
    "\n",
    "\t\t\tkernel_initializer=tf.truncated_normal_initializer())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def BN_ReLU(inputs, training):\n",
    "\n",
    "\t\"\"\"Performs a batch normalization followed by a ReLU6.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\t# We set fused=True for a significant performance boost. See\n",
    "\n",
    "\t# https://www.tensorflow.org/performance/performance_guide#common_fused_ops\n",
    "\n",
    "\tinputs = tf.layers.batch_normalization(\n",
    "\n",
    "\t\t\t\tinputs=inputs,\n",
    "\n",
    "\t\t\t\taxis=-1,\n",
    "\n",
    "\t\t\t\tmomentum=0.997,\n",
    "\n",
    "\t\t\t\tepsilon=1e-5,\n",
    "\n",
    "\t\t\t\tcenter=True,\n",
    "\n",
    "\t\t\t\tscale=True,\n",
    "\n",
    "\t\t\t\ttraining=training, \n",
    "\n",
    "\t\t\t\tfused=True)\n",
    "\n",
    "\n",
    "\n",
    "\treturn tf.nn.relu6(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    \n",
    "    def __init__(self,num_classes,num_filters,network_depth,block_sizes,block_strides):\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.num_filters = num_filters\n",
    "        self.block_sizes = [1] * block_sizes\n",
    "        self.block_strides = [1] + [2] * (block_strides - 1)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def __call__(self, inputs, training):\n",
    "\n",
    "        \n",
    "        \n",
    "        return self._build_network(inputs, training, self.block_sizes, self.block_strides)\n",
    "    \n",
    "    def _build_network(self,inputs, training, block_sizes, block_strides):\n",
    "\n",
    "        \n",
    "        inputs = Conv3D(inputs=inputs,\n",
    "                       filters=self.num_filters,\n",
    "                       kernel_size=3,\n",
    "                       strides=1)\n",
    "        \n",
    "        inputs = tf.identity(inputs,'initial_conv')\n",
    "        \n",
    "        skip_inputs = []\n",
    "        \n",
    "        for i, num_blocks in enumerate(self.block_sizes):\n",
    "            \n",
    "            \n",
    "            num_filters = self.num_filters * (2**i)\n",
    "            inputs = self._encoding_block_layer (\n",
    "                                                inputs=inputs, \n",
    "                                              filters=num_filters,\n",
    "                                                block_fn=self._residual_block,\n",
    "                                             blocks=num_blocks,\n",
    "                                     strides=self.block_strides[i],\n",
    "                                         training=training,\n",
    "                                          name='encode_block_layer{}'.format(i+1))\n",
    "                \n",
    "            skip_inputs.append(inputs)\n",
    "        \n",
    "        \n",
    "        for i, num_blocks in reversed(list(enumerate(self.block_sizes[1:]))):\n",
    "            num_filters = self.num_filters * (2**i)\n",
    "            inputs = self._decoding_block_layer(inputs=inputs,\n",
    "                                                    skip_inputs=skip_inputs[i],\n",
    "                                                    filters=num_filters,\n",
    "                                                    block_fn=self._residual_block,\n",
    "                                                    blocks=1,\n",
    "                                                    strides=self.block_strides[i+1],\n",
    "                                                    training=training,\n",
    "                                                    name='decode_block_layer{}'.format(len(self.block_sizes)-i-1))\n",
    "        \n",
    "        inputs= self._output_block_layer(inputs=inputs, training=training)\n",
    "        \n",
    "                \n",
    "        return inputs \n",
    "                \n",
    "            \n",
    "    def _output_block_layer(self,inputs,training):\n",
    "        \n",
    "        inputs = BN_ReLU(inputs, training)\n",
    "                \n",
    "        inputs = tf.layers.dropout(inputs,rate=0.5, training=training)\n",
    "                \n",
    "        inputs = Conv3D( inputs=inputs,\n",
    "                               filters= self.num_classes,\n",
    "                               kernel_size=1,\n",
    "                               strides=1,\n",
    "                               use_bias=True)\n",
    "                \n",
    "                \n",
    "        return tf.identity(inputs, 'output')\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "    def _encoding_block_layer(self, inputs,filters, block_fn,blocks, strides,training,name):\n",
    "        \n",
    "        def projection_shortcut(inputs):\n",
    "            return Conv3D(       inputs=inputs,\n",
    "                                 filters=filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides   ) \n",
    "        inputs = block_fn(inputs, filters, training, projection_shortcut, strides)\n",
    "                \n",
    "        for _ in range(1,blocks):\n",
    "            inputs = block_fn(inputs,filters, training, None, 1)\n",
    "                \n",
    "                \n",
    "        return tf.identity(inputs, name)\n",
    "                \n",
    "                \n",
    "                \n",
    "    def _decoding_block_layer(self, inputs, skip_inputs, filters,block_fn,blocks, strides, training, name):\n",
    "        inputs = Deconv3D(inputs=inputs,\n",
    "                                 filters=filters,\n",
    "                                 kernel_size=3,\n",
    "                                 strides=strides)\n",
    "                \n",
    "        inputs = inputs + skip_inputs\n",
    "                \n",
    "                \n",
    "        for _ in range(0, blocks):\n",
    "            inputs = block_fn(inputs, filters, training, None,1)\n",
    "                \n",
    "        return tf.identity(inputs,name)\n",
    "\n",
    "    def _residual_block(self, inputs, filters, training,\n",
    "                        projection_shortcut, strides):\n",
    "        shortcut = inputs\n",
    "        inputs = BN_ReLU(inputs, training)\n",
    "                \n",
    "                \n",
    "                \n",
    "        if projection_shortcut is not None:\n",
    "            shortcut = projection_shortcut(inputs)\n",
    "                \n",
    "                \n",
    "        inputs = Conv3D(inputs=inputs,\n",
    "                               filters=filters,\n",
    "                               kernel_size=3,\n",
    "                               strides=strides)\n",
    "                \n",
    "        inputs = BN_ReLU(inputs, training)\n",
    "                \n",
    "        inputs = Conv3D(inputs=inputs,\n",
    "                               filters=filters,\n",
    "                               kernel_size=3,\n",
    "                               strides=1)\n",
    "                \n",
    "                \n",
    "        return inputs + shortcut \n",
    "                \n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "                \n",
    "                \n",
    "            \n",
    "                \n",
    "            \n",
    "                \n",
    "                \n",
    "                \n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _model_fn(features, labels, mode):\n",
    "    \n",
    "    num_classes=39\n",
    "    num_filters=32\n",
    "    network_depth=3\n",
    "    block_sizes=3\n",
    "    block_strides=3\n",
    "    Training=True\n",
    "    \n",
    "    \n",
    "    net = Network(num_classes,num_filters,network_depth,block_sizes,block_strides)\n",
    "    logits = net(features, Training)\n",
    "\n",
    "    predictions = {\n",
    "        'classes' : tf.argmax(logits, axis=-1),\n",
    "        'probabilities':tf.nn.softmax(logits, name='softmax_tensor')}\n",
    "    print('We in')\n",
    "\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "\n",
    "\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "\n",
    "\n",
    "    tf.identity(cross_entropy, name='cross_entropy')\n",
    "    tf.summary.scalar('cross_entropy',cross_entropy)\n",
    "\n",
    "\n",
    "    loss = cross_entropy + 2e-6 * tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if 'kernel' in v.name])\n",
    "    print('we in')\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        \n",
    "        print('We in')\n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_op = optimizer.minimize(loss, global_step)\n",
    "\n",
    "    else:\n",
    "        train_op = None\n",
    "\n",
    "    accuracy = tf.metrics.accuracy(labels, predictions['classes'])\n",
    "    metrics = {'accuracy':accuracy}\n",
    "\n",
    "\n",
    "    tf.identity(accuracy[1],name='train_accuracy')\n",
    "    tf.summary.scalar('train_accuracy', accuracy[1])\n",
    "    print(metrics)\n",
    "\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                     predictions=predictions,\n",
    "                                     loss=loss,\n",
    "                                     train_op=train_op,\n",
    "                                     eval_metric_ops=metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training step / Validation step (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting parameters for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=39 #the number of classes\n",
    "network_depth=3 #the network depth\n",
    "num_filters=32 #the initial numbers of filter for the first convolution block\n",
    "block_sizes=3 \n",
    "block_strides=3\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'#enable gpu \n",
    "os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'\n",
    "num_training_subs=6\n",
    "train_epochs=100000\n",
    "epochs_per_eval=5000\n",
    "data_dir=r\"C:\\Users\\r778e\\Desktop\\tfrecord\" #the directory where the input data is stored as TFRecords file\n",
    "model_dir=r'C:\\Users\\r778e\\Desktop\\modelsave_test'#the directory where you the model is saved\n",
    "batch_size=5\n",
    "validation_id=3 #the patient used for the validation\n",
    "num_parallel_callszer=1 \n",
    "overlap_step=20\n",
    "patch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mode=tf.estimator.ModeKeys.TRAIN \n",
    "training=True\n",
    "\n",
    "\n",
    "save_checkpoints_steps = 5000 * \\\n",
    "                 num_training_subs // batch_size\n",
    "\n",
    "run_config = tf.estimator.RunConfig().replace(\n",
    "    save_checkpoints_steps=save_checkpoints_steps,\n",
    "    keep_checkpoint_max=0)\n",
    "\n",
    "\n",
    "classifer = tf.estimator.Estimator(\n",
    "    model_fn=_model_fn,\n",
    "    model_dir=model_dir,\n",
    "    config=run_config\n",
    ")\n",
    "\n",
    "\n",
    "for _ in range(train_epochs // epochs_per_eval):\n",
    "    tensor_to_log = {\n",
    "        'cross_entropy':'cross_entropy',\n",
    "        'train_accuracy':'train_accuracy'\n",
    "    }\n",
    "\n",
    "    logging_hook = tf.train.LoggingTensorHook(tensors=tensor_to_log, every_n_iter=100)\n",
    "\n",
    "    print('Starting a training cycle zerrrrr!')\n",
    "\n",
    "    def input_fn_train():\n",
    "        return input_function( data_dir=data_dir,\n",
    "                               mode='train',\n",
    "                               patch_size=patch_size, \n",
    "                                    batch_size=batch_size,\n",
    "                                    buffer_size=num_training_subs,\n",
    "                                    valid_id=validation_id,\n",
    "                                    pred_id=-1, # not used\n",
    "                                    overlap_step=-1, # not used\n",
    "                                    num_epochs=epochs_per_eval,\n",
    "                                    num_parallel_calls=num_parallel_callszer)\n",
    "\n",
    "    classifer.train(input_fn=input_fn_train, hooks=[logging_hook])\n",
    "    \n",
    "    \n",
    "#    if self.conf.validation_id != -1:\n",
    "#        print('Starting to evaluate.')\n",
    "#\n",
    "#        def input_fn_eval():\n",
    "#            return input_function(\n",
    "#            data_dir=data_dir,\n",
    "#            mode='valid',\n",
    "#            patch_size=patch_size,\n",
    "#            batch_size=batch_size,\n",
    "#            buffer_size=-1, # not used\n",
    "#            valid_id=validation_id,\n",
    "#            pred_id=-1, # not used\n",
    "#            overlap_step=overlap_step,\n",
    "#            num_epochs=1,\n",
    "#            num_parallel_calls=num_parallel_calls)\n",
    "#\n",
    "#        classifier.evaluate(input_fn=input_fn_eval)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(raw_data_dir,data_dir,model_dir,num_parallel_calls,save_dir,checkpoint_num,prediction_id):\n",
    "    os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'\n",
    "    mode=tf.estimator.ModeKeys.PREDICT\n",
    "    \n",
    "    print(50*'-')\n",
    "    print('Perform prediction for subject-%d:' % prediction_id)\n",
    "\n",
    "    print('loading data...')\n",
    "    print(50*'-')    \n",
    "\n",
    "    [ct,_] = load_subject(raw_data_dir, prediction_id)\n",
    "\n",
    "    (_,cut_size) = cut_edge(ct)\n",
    "    print('Check cut_size:', cut_size)\n",
    "\n",
    "    cutted_CT = ct[cut_size[0]:cut_size[1],cut_size[2]:cut_size[3],cut_size[4]:cut_size[5],:]\n",
    "    patch_ids = prepare_validation(cutted_CT,patch_size,overlap_step)\n",
    "    num_patchs = len(patch_ids)\n",
    "    print('Numbers of patches:', num_patchs)\n",
    "\n",
    "\n",
    "    print('Initialize')\n",
    "    classifier = tf.estimator.Estimator(\n",
    "        model_fn=_model_fn,\n",
    "        model_dir=model_dir)\n",
    "\n",
    "\n",
    "\n",
    "    def input_fn_predict():\n",
    "            return input_function( data_dir=data_dir,\n",
    "                                    mode='pred',\n",
    "                                    patch_size=patch_size,\n",
    "                                    batch_size=batch_size,\n",
    "                                    buffer_size=-1,\n",
    "                                    valid_id=-1,\n",
    "                                    pred_id=prediction_id, # not used\n",
    "                                    overlap_step=overlap_step, # not used\n",
    "                                    num_epochs=1,\n",
    "                                    num_parallel_calls=num_parallel_calls)\n",
    "\n",
    "\n",
    "    checkpoint_file = os.path.join(model_dir,'model.ckpt-'+str((checkpoint_num)))\n",
    "\n",
    "\n",
    "    preds = classifier.predict(\n",
    "            input_fn=input_fn_predict,\n",
    "        checkpoint_path=checkpoint_file)\n",
    "\n",
    "\n",
    "    print('Starting to predict')\n",
    "\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    for i, pred in enumerate(preds):\n",
    "            location = patch_ids[i]\n",
    "\n",
    "            print('Step {:d}/{:d} processing results for ({:d},{:d},{:d})'.format(\n",
    "\n",
    "            i+1, num_patchs, location[0], location[1], location[2]),\n",
    "            end='\\r',\n",
    "\n",
    "            flush=True)\n",
    "\n",
    "            logits = pred['probabilities']\n",
    "\n",
    "            for j in range(patch_size):\n",
    "\n",
    "                    for k in range(patch_size):\n",
    "\n",
    "                            for l in range(patch_size):    \n",
    "\n",
    "                                    key = (location[0]+j, location[1]+k, location[2]+l)\n",
    "\n",
    "                                    if key not in predictions.keys():\n",
    "\n",
    "                                                predictions[key] = []\n",
    "\n",
    "                                    predictions[key].append(logits[j, k, l, :])\n",
    "\n",
    "\n",
    "\n",
    "    print('Averaging results...')\n",
    "\n",
    "\n",
    "\n",
    "    results = np.zeros((ct.shape[0], ct.shape[1], ct.shape[2], num_classes),\n",
    "\n",
    "    dtype=np.float32)\n",
    "\n",
    "    print(results.shape)\n",
    "\n",
    "    for key in predictions.keys():\n",
    "        results[cut_size[0]+key[0],cut_size[2]+key[1], cut_size[4]+key[2]] = np.mean(predictions[key], axis=0)\n",
    "\n",
    "    results = np.argmax(results, axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "    print('Saving results...')\n",
    "\n",
    "\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "    save_filename = 'preds-' + str(checkpoint_num) + '-sub-' + str(prediction_id) + '-overlap-' + str(overlap_step) + '-patch-' + str(patch_size) + '.npy'\n",
    "\n",
    "    save_file = os.path.join(save_dir, save_filename)\n",
    "\n",
    "    np.save(save_file, results)\n",
    "\n",
    "\n",
    "\n",
    "    print('Done.')\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "raw_data_dir=r\"C:\\Users\\r778e\\Desktop\\Data_to_aug\\data_set_no_augmentation\" #the directory where the raw data is stored\n",
    "data_dir=r\"C:\\Users\\r778e\\Desktop\\tfrecord\" #the directory where the input data is stored as TFRecords file\n",
    "model_dir=r'C:\\Users\\r778e\\Desktop\\KnowUbone\\model_save_64_patch_16_overlap_1' #the directory where you saved your model\n",
    "num_parallel_calls=1\n",
    "save_dir=r'C:\\Users\\r778e\\Desktop\\save prediction' #The directory where you want to save your prediction\n",
    "checkpoint_num=60000\n",
    "prediction_id=5\n",
    "predict(raw_data_dir,data_dir,model_dir,num_parallel_calls,save_dir,checkpoint_num,prediction_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the npa prediction and convert to mha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prediction(save_dir,prediction_id,overlap_step,patch_size,checkpoint_num):\n",
    "    save_filename = 'preds-' + str(checkpoint_num) + '-sub-' + str(prediction_id) + '-overlap-' + str(overlap_step) + '-patch-' + str(patch_size) + '.npy'\n",
    "    save_file = os.path.join(save_dir, save_filename)\n",
    "    prediction=np.load(save_file)\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=load_prediction(save_dir,prediction_id,overlap_step,patch_size,checkpoint_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_npa_to_mha_and_npa(prediction,id_patient,output_dir):\n",
    "    \n",
    "    \"\"\"Purpose: convert the npa prediction to the conform mha file.\n",
    "    \n",
    "        Args:\n",
    "            prediction:\n",
    "            id_patient:\n",
    "            outputdir_dir:\n",
    "    \n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    npa_res=prediction.astype('int16')\n",
    "    res_img = sitk.GetImageFromArray(npa_res)\n",
    "    \n",
    "    npa_zeros=np.zeros([512,512],'int16')\n",
    "    npa_zeros=npa_zeros[np.newaxis,:,:]\n",
    "    \n",
    "    filename_ground_truth=r\"D:\\xxx_gathering_augmented\\HeadAndNeck_anonymous-%i.mha\" %id_patient\n",
    "    ground_truth = sitk.ReadImage(filename_ground_truth)\n",
    "    \n",
    "    if len(res_img) == ground_truth.GetDepth():\n",
    "        res_img.CopyInformation(ground_truth)\n",
    "        sitk.WriteImage(res_img,os.path.join(output_dir,\"prediction_from_patient_%i.mha\"%id_patient))\n",
    "        \n",
    "        print('Done.')\n",
    "        print('The prediction has been succesfuly converted to mha file')\n",
    "    \n",
    "    else:\n",
    "        npa_res=np.concatenate((npa_res,npa_zeros),axis=0)\n",
    "        res_img = sitk.GetImageFromArray(npa_res)\n",
    "        res_img.CopyInformation(ground_truth)\n",
    "        sitk.WriteImage(res_img,os.path.join(output_dir,\"prediction_from_patient_%i.mha\"%id_patient))\n",
    "       \n",
    "        print('Done.')\n",
    "        print('The prediction has been succesfuly converted to mha file and npa file')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_patient=5\n",
    "output_dir=\"\"\n",
    "convert_to_mha(prediction,id_patient,output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
